
# using Pkg; Pkg.activate(joinpath(@__DIR__(), "..", ".."))
# using TestEnv; TestEnv.activate();

##

using Test, ACEbase 
using Polynomials4ML.Testing: print_tf, println_slim

using ACEpotentials
M = ACEpotentials.Models

using Optimisers, ForwardDiff

using Random, LuxCore, StaticArrays, LinearAlgebra
rng = Random.MersenneTwister(1234)
Random.seed!(11)

##

elements = (:Si, :O)
level = M.TotalDegree()
max_level = 15
order = 3 

##

@info("Test ybasis of the Model is used correctly")
msolid = M.ace_model(; elements = elements, order = order, Ytype = :solid, 
            level = level, max_level = max_level, maxl = 8, pair_maxn = 15, 
            init_WB = :glorot_normal, init_Wpair = :glorot_normal)
mspherical = M.ace_model(; elements = elements, order = order, Ytype = :spherical, 
            level = level, max_level = max_level, maxl = 8, pair_maxn = 15, 
            init_WB = :glorot_normal, init_Wpair = :glorot_normal)
ps, st = LuxCore.setup(rng, msolid)

for ntest = 1:30 
   ùê´ = randn(SVector{3, Float64})
   Ysolid = msolid.ybasis(ùê´)
   Yspher = mspherical.ybasis(ùê´)
   ll = [ M.P4ML.SpheriCart.idx2lm(i)[1] for i in 1:length(Ysolid) ]
   print_tf(@test (Yspher .* (norm(ùê´)).^ll) ‚âà Ysolid)
end
println() 

##

for ybasis in [:spherical, :solid]
   # ybasis = :solid
   @info("=== Testing ybasis = $ybasis === ")
   local ps, st, Nat, model 
   model = M.ace_model(; elements = elements, order = order, Ytype = ybasis, 
                        level = level, max_level = max_level, maxl = 8, 
                        pair_maxn = 15, 
                        init_WB = :glorot_normal, init_Wpair = :glorot_normal)

   ps, st = LuxCore.setup(rng, model)

##

   @info("Test Rotation-Invariance of the Model")

   for ntest = 1:50 
      local st1, Nat, Rs, Zs, Z0, val 

      Nat = rand(8:16)
      Rs, Zs, Z0 = M.rand_atenv(model, Nat)
      val = M.evaluate(model, Rs, Zs, Z0, ps, st)

      p = shuffle(1:Nat)
      Rs1 = Ref(M.rand_iso()) .* Rs[p]
      Zs1 = Zs[p]
      val1 = M.evaluate(model, Rs1, Zs1, Z0, ps, st)

      print_tf(@test abs(val - val1) < 1e-10)
   end
   println()

##

   @info("Test derivatives w.r.t. positions")
   Rs, Zs, z0 = M.rand_atenv(model, 16)
   Ei = M.evaluate(model, Rs, Zs, z0, ps, st)
   Ei1, ‚àáEi = M.evaluate_ed(model, Rs, Zs, z0, ps, st)
   println_slim(@test Ei ‚âà Ei1)

   for ntest = 1:20 
      local Nat, Rs, Zs, z0, Us, F, dF
      Nat = rand(8:16)
      Rs, Zs, z0 = M.rand_atenv(model, Nat)
      Us = randn(SVector{3, Float64}, Nat)
      F(t) = M.evaluate(model, Rs + t * Us, Zs, z0, ps, st)
      dF(t) = dot(M.evaluate_ed(model, Rs + t * Us, Zs, z0, ps, st)[2], Us)
      print_tf(@test ACEbase.Testing.fdtest(F, dF, 0.0; verbose=false))
   end
   println() 

##

   @info("Test derivatives w.r.t. parameters")
   Nat = 15
   Rs, Zs, z0 = M.rand_atenv(model, Nat)
   Ei = M.evaluate(model, Rs, Zs, z0, ps, st)
   Ei1, ‚àáEi = M.grad_params(model, Rs, Zs, z0, ps, st)
   println_slim(@test Ei ‚âà Ei1)

   for ntest = 1:20
      local Nat, Rs, Zs, z0, pvec, uvec, F, dF0, _restruct 

      Nat = rand(8:16)
      Rs, Zs, z0 = M.rand_atenv(model, Nat)
      pvec, _restruct = destructure(ps)
      uvec = randn(length(pvec)) / sqrt(length(pvec))
      F(t) = M.evaluate(model, Rs, Zs, z0, _restruct(pvec + t * uvec), st)
      dF0 = dot( destructure( M.grad_params(model, Rs, Zs, z0, ps, st)[2] )[1], uvec )
      print_tf(@test ACEbase.Testing.fdtest(F, t -> dF0, 0.0; verbose = false))
   end
   println() 

##

   @info("Test second mixed derivatives reverse-over-reverse")
   for ntest = 1:20 
      local Nat, Rs, Zs, Us, Ei, ‚àÇEi, ‚àÇ2_Ei, 
            ps_vec, vs_vec, F, dF0, z0, _restruct 

      Nat = rand(8:16)
      Rs, Zs, z0 = M.rand_atenv(model, Nat)
      Us = randn(SVector{3, Float64}, Nat)
      Ei = M.evaluate(model, Rs, Zs, z0, ps, st)
      Ei, ‚àÇEi = M.grad_params(model, Rs, Zs, z0, ps, st)

      # test partial derivative w.r.t. the Ei component 
      ‚àÇ2_Ei = M.pullback_2_mixed(1.0, 0*Us, model, Rs, Zs, z0, ps, st)
      print_tf(@test destructure(‚àÇ2_Ei)[1] ‚âà destructure(‚àÇEi)[1])

      # test partial derivative w.r.t. the ‚àáEi component 
      ‚àÇ2_‚àáEi = M.pullback_2_mixed(0.0, Us, model, Rs, Zs, z0, ps, st)
      ‚àÇ2_‚àáEi_vec = destructure(‚àÇ2_‚àáEi)[1]

      ps_vec, _restruct = destructure(ps)
      vs_vec = randn(length(ps_vec)) / sqrt(length(ps_vec))
      F(t) = dot(Us, M.evaluate_ed(model, Rs, Zs, z0, _restruct(ps_vec + t * vs_vec), st)[2])
      dF0 = dot(‚àÇ2_‚àáEi_vec, vs_vec)
      print_tf(@test ACEbase.Testing.fdtest(F, t -> dF0, 0.0; verbose=false))
   end
   println() 

##

   @info("Test basis implementation")

   for ntest = 1:30 
      local Nat, Rs, Zs, z0, Ei, B, Œ∏, st1 , ‚àáEi

      Nat = 15
      Rs, Zs, z0 = M.rand_atenv(model, Nat)
      i_z0 = M._z2i(model, z0)
      Ei = M.evaluate(model, Rs, Zs, z0, ps, st)
      B = M.evaluate_basis(model, Rs, Zs, z0, ps, st)
      Œ∏ = M.get_basis_params(model, ps)
      print_tf(@test Ei ‚âà dot(B, Œ∏))

      Ei, ‚àáEi = M.evaluate_ed(model, Rs, Zs, z0, ps, st)

      B1, ‚àáB = M.evaluate_basis_ed(model, Rs, Zs, z0, ps, st)
      print_tf(@test B ‚âà B1)
      print_tf(@test ‚àáEi ‚âà sum(Œ∏ .* ‚àáB, dims=1)[:])   
   end
   println() 

##

   @info("Test the full mixed jacobian")

   for ntest = 1:30 
      local Nat, Rs, Zs, z0, Ei, ‚àáEi, ‚àÇ‚àÇEi, Us, F, dF0

      Nat = 15
      Rs, Zs, z0 = M.rand_atenv(model, Nat)
      Us = randn(SVector{3, Float64}, Nat) / sqrt(Nat)
      F(t) = destructure( M.grad_params(model, Rs + t * Us, Zs, z0, ps, st)[2] )[1]
      dF0 = ForwardDiff.derivative(F, 0.0)
      ‚àÇ‚àÇEi = M.jacobian_grad_params(model, Rs, Zs, z0, ps, st)[3]
      print_tf(@test dF0 ‚âà transpose.(‚àÇ‚àÇEi) * Us)
   end 
   println() 


##

   @info("check splinification")
   lin_ace = M.splinify(model, ps; nnodes = 1000)
   ps_lin, st_lin = LuxCore.setup(rng, lin_ace)
   ps_lin.WB[:] .= ps.WB[:] 
   ps_lin.Wpair[:] .= ps.Wpair[:]

   for ntest = 1:10
      local len, Nat, Rs, Zs, z0, Ei 
      len = 100 
      mae = sum(1:len) do _
         Nat = rand(8:16)
         Rs, Zs, z0 = M.rand_atenv(model, Nat)
         Ei = M.evaluate(model, Rs, Zs, z0, ps, st)
         Ei_lin = M.evaluate(lin_ace, Rs, Zs, z0, ps_lin, st_lin)
         abs(Ei - Ei_lin)
      end
      mae /= len 
      print_tf(@test mae < 0.01)
   end
   println() 

## 

   @info("After splinification check correctness of evaluate_basis_ed again")
   for ntest = 1:10
      local Nat, Rs, Zs, z0, Ei 
      Nat = rand(8:16)
      Rs, Zs, z0 = M.rand_atenv(model, Nat)
      Us = randn(SVector{3, Float64}, Nat) / sqrt(Nat)
      B, ‚àÇB = M.evaluate_basis_ed(lin_ace, Rs, Zs, z0, ps, st)
      B0 = M.evaluate_basis(lin_ace, Rs, Zs, z0, ps, st)
      ‚àÇB0 = M.jacobian_grad_params(lin_ace, Rs, Zs, z0, ps, st)[3]
      print_tf(@test B ‚âà B0)
      print_tf(@test ‚àÇB ‚âà ‚àÇB0)
   end
   println() 
      # ‚àÇE0 = ForwardDiff.derivative(
      #       t -> M.evaluate_basis(lin_ace, Rs + t * Us, Zs, z0, ps, st), 
      #       0.0)
      
end

##

#=
@info("Basic performance benchmarks")
# first test shows the performance is not at all awful even without any 
# optimizations and reductions in memory allocations. 
using BenchmarkTools
Nat = 15
Rs, Zs, z0 = M.rand_atenv(model, Nat)
Us = randn(SVector{3, Float64}, Nat)

@info("Evaluation and adjoints")
print("   evaluate : "); @btime M.evaluate($model, $Rs, $Zs, $z0, $ps, $st)
print("evaluate_ed : "); @btime M.evaluate_ed($model, $Rs, $Zs, $z0, $ps, $st)
print("grad_params : "); @btime M.grad_params($model, $Rs, $Zs, $z0, $ps, $st)
print("  reverse^2 : "); @btime M.pullback_2_mixed(rand(), $Us, $model, $Rs, $Zs, $z0, $ps, $st)

@info("Basis evaluation ")
@info("  NB: this is currently implemented using ForwardDiff and likely inefficient")
print("      evaluate_basis : "); @btime M.evaluate_basis($model, $Rs, $Zs, $z0, $ps, $st)
print("   evaluate_basis_ed : "); @btime M.evaluate_basis_ed($model, $Rs, $Zs, $z0, $ps, $st)
print("jacobian_grad_params : "); @btime M.jacobian_grad_params($model, $Rs, $Zs, $z0, $ps, $st)
=#
